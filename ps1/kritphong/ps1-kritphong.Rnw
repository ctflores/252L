\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{lineno}
\usepackage{graphicx}
\usepackage{fancyvrb}

\linenumbers

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{Problem Set 1}
\author{Kritphong Monghonvanit}

\maketitle

\section{Bernoulli Random Variables}

\begin{enumerate}
	\item
	The correlation matrix has 1's along the diagonal, which makes sense
	because:

	\[ \sigma(x,x)=1 \]

	The matrix is also symmetric about the diagonal, which makes sense
	because:

	\[ \sigma(x,y) = \sigma(y,x) \]

	The correlations are also relatively small, which makes sense because
	the data is generated randomly.

<<echo=FALSE>>=
set.seed(12311)
x1<-matrix(rbinom(1000,1,.5),100,10)
cor(x1)
@

	\item
	The variance of the row sums is \Sexpr{var(rowSums(x1))}.  Given the context of the
	following question, this represents variance in total test scores across
	individuals.

	\item
	Probably not, because the correlations are so small. One might also
	expect a test to have easy and hard questions, which this randomly
	constructed matrix does not indicate when looking at the column sums.

<<echo=FALSE>>=
set.seed(12311)
th<-matrix(rnorm(100),100,10,byrow=FALSE)
diff<-matrix<-matrix(rnorm(10),100,10,byrow=TRUE)
kern<- exp(th - diff)
pr<-kern/(1+kern)
test<-matrix(runif(1000),100,10)
x2<-ifelse(pr>test,1,0)
@

	\item
	For $x_2$, the matrix still has 1's along the diagonal and is symmetric
	about the diagonal, but generally speaking the correlations are higher
	than they were in $x_1$.

	\item
	The variance of the row sums is \Sexpr{var(rowSums(x2))}. The variation
	in row sums here is higher. If you look at histograms of the row sums,
	the data is also somewhat more uniformly distributed in $x_2$ than in
	$x_1$, so it makes sense that the variance would be higher in this
	situation.

	\item
	Considering the column sums, it's clear that there are items that are
	more difficult (20 correct responses) and less difficult (69 or 73
	correct responses), so in this sense, it is more realistic.
\end{enumerate}

<<echo=FALSE>>=
par(mfrow=c(2,2))
hist(rowSums(x1))
hist(rowSums(x2))
hist(colSums(x1))
hist(colSums(x2))
@

Looking at histograms of row sums in $x_1$ and $x_2$, $x_2$ is less clearly
normally distributed, meaning that there are respondents at lower ability
levels, but a sharper drop off after 7 (and 8) correct.  More than anything, the
difference in the variance between the column sums (61.7 in $x_1$ and 242.7 in
$x_2$) implies that all the items in $x_1$ would have been of similar
difficulty, whereas the items in $x_2$ are of varying difficulty.  In practice,
it's hard to write items that are of similar difficulty and result in
respondents of similar scores have wildly different response profiles.

\section{Logistic Regression}

\begin{enumerate}
	\item
	The association between $x$ and $y_2$ is stronger than the association
	between x and $y_1$. This is because the AIC, null deviance and residual
	deviance are lower for $y_2$. In addition, when looking at plots, there
	is less overlap between the top and bottom sections in the plots of
	$y_2$ than in $y_1$. In $y_2$, the logistic curve has a steeper
	transition from minimum value to maximum value.

	\item
	The regression coefficients, $\beta_0$ and $\beta_1$, are related to the
	shape of the logistic fit. Specifically, $\beta_0$ is related to the
	intercept. $\beta_1$ is a measure of test item discrimination. $\beta_1$
	is related to the slope at the point of inflection, so a higher slope
	would indicate higher levels of discrimination. $\beta_0$ is related to
	difficulty, with a lower $\beta_0$ indicating an easier test item.
	$\beta_0$ is the odds of getting an answer correct when $x = 0$.

	$$P(x=0) = \frac{e^{\beta_0}}{(1+e^{\beta_0})}$$

	\input{table_glm_models.tex}

	\item
	Do m1 and m2 show equivalent model fit? Can you notice anything peculiar
	about either y1 or y2 (in terms of their association with x)?
\end{enumerate}

\section{Likelihood}

\begin{enumerate}
	\item
	Depending on the randomly generated data, the position on the $x$-axis
	of the maxima of the likelihood graph changes slightly, but it is always
	in the neighborhood of $x=0$.

	\includegraphics[width=0.5\textwidth]{ps1-like-plot}

	\item
	As the sample size increases, the mean approaches zero and the variance
	approaches one.

	\begin{tabular}{|l|l|l|}
		\hline
		Sample Size & Mean & Variance \\
		\hline
		10 & 0.2335989 & 1.3643526 \\
		100 & 0.1675501 & 0.9539125 \\
		1000 & -0.02798667 & 0.95039213 \\
		10000 & 0.0014444 & 1.0368603 \\
		\hline
	\end{tabular}

	\item
	I notice what appears to be a normal distribution with points densely
	aligned at y=0 and more sparsely distributed around the mean. The mean
	is between -0.5 and 0, and the standard deviation is small. The y-axis
	is labeled in scientific notation, indicating that the numbers are
	extremely small.

	\item
	The original estimates for the mean and variance were -.1686 and .84216,
	respectively. As the sample size decreased, the estimates for variance
	became less accurate: pars == (-0.15, 1.519). As the sample size
	increased, the estimates for mean and variance converged on the standard
	parameters of 0 and 1, respectively. At n=10000, pars == (0.00213,
	0.9787).
\end{enumerate}

\section{Item Quality}

\begin{enumerate}
	\item
	In my response, I will refer to item-total correlations as
	predictability and p-values as difficulty. The items in the first
	dataset had a wide range of predictability and difficulty. The most
	predictable items, that is, the items with the highest item-total
	correlations, were in the mid-high range of difficulty, but not all of
	the items in this range were very predictable. The range of
	predictability was near 0 to 0.6. In general, the lowest and highest
	difficulty items were not very predictable of the overall measure.

	When comparing the two datasets, the second set of questions had higher
	predictability overall and throughout a wider range of difficulties of
	questions. The ranges of difficulty of the questions were about the same
	between the sets, but more of the questions in the second set had many
	more items with predictability between .35 and .5. In both cases, the
	easiest and most difficult questions were the least predictive.
\end{enumerate}

\section{Buffon's Needle Problem}

To test the Buffon's Needle Problem we simulated a large number of needles and
defined the length of the needles and the spaces to be equal (2 in both cases).

Then, we randomized the position each needle could land by simulating the
orientation of one of its points in a circle.  Considering that the relationship
of the length of the needle and the space it takes once landed depends on the
angle the needle lands, we simulated the positions of each needle and tested
whether is hit a line.

Our conclusion is that in a scenario that \texttt{l} and \texttt{d} are the same
length, the chance of a needle hit a line is approximately 0.63.

<<>>=
n <- 1000000
d <- 2
l <- 2

orientation <- matrix(NA,n,4)

orientation[,1] <- runif(n, 0, d)
orientation[,2] <- runif(n, -pi/2, pi/2)
orientation[,3] <- orientation[,1] + (l * cos(orientation[,2]))
orientation[,4] <- ifelse(orientation[,3] >= d,1,0)

sum(orientation[,4])/n
@

\end{document}
