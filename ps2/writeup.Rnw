\documentclass{article}

\title{Problem Set 2}
\author{Kaylee, Carrie, Kritphong, Filipe and Klint \\ EDUC 252L}

<<echo=FALSE>>=
knitr::opts_chunk$set(cache=TRUE)
@

\begin{document}
\maketitle
\section{Breaking the Classical Test Theory Model}

  \subsection{Coin Flips}
    Coin flips should not be reliable data - they're random!  To look at this a little more analytically:
    
      \[ \alpha = \frac{K}{K-1}\Bigg(1-\frac{\sum_{i=1}^{K}p_i(1-p_i)}{\sigma_{X}^{2}}\Bigg)\]
    
    The interesting thing to note here is that the probability of flipping heads is:
      \[p_i = 0.5 \]
    And the variance on the sum of $K$ coin flips will be:
      \[ \sigma_X = 0.25K \]
    Substituting in the formula for Cronbach's Alpha:
      \[ \alpha = \frac{K}{K-1}\Bigg(1-\frac{\sum_{i=1}^{K}(0.5)(1-0.5)}{0.25K}\Bigg)\]
    Cleaning up:
      \[ \alpha = \frac{K}{K-1}\Bigg(1-\frac{0.25K}{0.25K}\Bigg)\]
      \[ \alpha = \frac{K}{K-1}(1-1)\]
      \[ \alpha = 0 \]
    The expectation, then, is that $\alpha$ shoudl be zero for each situation.


<<echo=FALSE>>=
source("https://raw.githubusercontent.com/ben-domingue/252L_winter2018/master/helper/cronbachs_alpha.R") 


par(mfrow=c(2,3),mgp=c(2,1,0),mar=c(3,3,2,1))
for (N.items in seq(20,50,by=15)) { 
    for (N.ppl in c(100,1000)) {
        alph<-numeric()
        for (i in 1:25) {
            resp<-matrix(rbinom(N.items*N.ppl,size=1,pr=.5),N.ppl,N.items)
            kr20(resp)->alph[i]
        }
        boxplot(alph,main=paste(N.items,N.ppl))
        abline(h=mean(alph))
    }
}
mtext(side=1,line=.5,"from coin flips")
@

    These $\alpha$ plots make sense - they are centered around zero, as predicted, and as the number of items increases, $\alpha$ is more tightly clustered around zero.

  \subsection{Simulating Item Response Data}

<<echo=FALSE, results='hide'>>=
###################################################################################################################
##now, we were kind of cheating with the coins. we didn't really impose any structure on the thing and this isn't really demonstrating something too weird about ctt, just that coin flips aren't reliable measures!

##let's give ourselves a stiffer challenge. let's generate item response data that has
##A. a pre-specified reliability
##B. and yet weird inter-workings such that reliability estimates fall apart completely
##C. [i don't really emphasize this in the below, but you can also pick (roughly) the distribution of item p-values]
##below i'm going to do just that using this function.
sim_ctt<-function(N.items, #N.items needs to be even
                  N.ppl,
                  s2.true,
                  s2.error,
                  check=FALSE #this will produce a figure that you can use to check that things are working ok
                  ) { 
    ##desired reliability
    reliability<-(s2.true)/(s2.error+s2.true)
    ##sample (unobserved) true abilities & errors
    T<-round(rnorm(N.ppl,mean=round(N.items/2),sd=sqrt(s2.true)))
    E<-round(rnorm(N.ppl,mean=0,sd=sqrt(s2.error)))
    O<-T+E
    ifelse(O<0,0,O)->O
    ifelse(O>N.items,N.items,O)->O
    ##note that we already know how many items a person got right (O)
    ##ctt doesn't specify how individual item responses are generated.
    ##so we have carte blance in terms of coming up with item responses. here's how i'm going to do it. 
    pr<-runif(N.items,min=.25,max=.85)
    pr.interval<-c(0,cumsum(pr))/sum(pr)
    resp<-list()
    for (i in 1:N.ppl) { #for each person
        resp.i<-rep(0,N.items) #start them with all 0s
        if (O[i]>0) { 
            for (j in 1:O[i]) { #now for each of the observed correct responses (for those who got at least 1 item right) i'll pick an item to mark correct
                init.val<-1
                while (init.val==1) {
                    index<-cut(runif(1),pr.interval,labels=FALSE) #here i pick the item wherein i weight the items by the "pr" argument that is random here but could be pre-specied by the user
                    init.val<-resp.i[index]
                }
                resp.i[index]<-1
            }
        }
        resp[[i]]<-resp.i
    }
    do.call("rbind",resp)->resp
    ##note that we are getting both the p-values and the true score/observed score relationships right!
    #if (check) {
    #    par(mfrow=c(2,1))
    #    plot(T,rowSums(resp),xlab="true score",ylab="observed scores"); abline(0,1)
    #    plot(pr,colMeans(resp),xlab="pr",ylab="observed p-values"); abline(0,1)
    #}
    obs.rel<-cor(T,O)^2 #we can use this to make sure that the true and observed score variances are right.
    print(c(reliability,obs.rel)) ##for real time checking, these are the pre-specified reliability and what we get from correlation our true and observed scores
    kr<-kr20(resp)
    c(obs.rel,kr)
}

##to illustrate the basic idea here, we'll generate 10 sets of item response data.
##for each iteration, we'll use the same parameters, these are below.
N.items<-50
N.ppl<-500
s2.true<-10
s2.error<-3
alph<-list()
for (i in 1:10) {
    alph[[i]]<-sim_ctt(N.items=N.items,N.ppl=N.ppl,s2.true=s2.true,s2.error=s2.error,check=TRUE) #note, the check option is goign to produce a figure that allows us to check the marginals to make sure things are looking ok
}
do.call("rbind",alph)->alph
plot(alph,xlim=c(0,1),ylim=c(-.2,1),pch=19,xlab="true correlation",ylab="kr20"); abline(0,1)
abline(v=s2.true/(s2.true+s2.error),col="red")
##what you can see here is that the item response datsets have the right property when it comes to the observed correlation between O and T (these correlations are the dots; the red line is the pre-specified reliability)
##but, kr20 is producing reliability estimates that are *way* low
##something is going disastrously awry.
##q what is it? make sure you see that the true/error variances are getting handled correctly (check the print statement and related output from sim_ctt)
##q: any clues as to why this is happening? what is the feature of my data generating mechanism that causes things to get all wacky?

@

The feature of the data generation mechanism that makes the $\alpha$ values super low is that items are getting marked correctly (essentially) at random!  Even though the item responses generated correct p-values and test-level correlations, the data generation disregarded any internal structure you would expect.  More clearly stated, respondents of similar ability levels did not have similar item response profiles.

<<echo=FALSE, results='hide'>>=

par(mfrow=c(3,3),mgp=c(2,1,0),mar=c(3,3,2,1))
##now let's look at reliabilities for many different datasets generated by the above wherein we vary the true and error variances
N.ppl<-500
N.items<-50
for (s2.true in N.items*c(.1,.5,.9)) {
    for (s2.error in N.items*c(.1,.25,.5)) {
        alph<-list()
        for (i in 1:10) {
            alph[[i]]<-sim_ctt(N.items=N.items,N.ppl=N.ppl,s2.true=s2.true,s2.error=s2.error,check=FALSE)
        }
        plot(do.call("rbind",alph),xlim=c(-1,1),ylim=c(-1,1),pch=19,xlab="true correlation",ylab="kr20")
        mtext(side=3,line=0,paste("s2.true",round(s2.true,2),"; s2.error",round(s2.error,2)),cex=.7)
        abline(0,1)
        abline(h=(s2.true)/(s2.error+s2.true),col="red",lty=2)
        abline(v=(s2.true)/(s2.error+s2.true),col="red",lty=2)
    }
}
##q. how does the kr20 estimate of reliability behave as a function of the true and error variances?
##q. what does this make you think of the CTT model? 
@

Looking at the resulting plots, it's clear that even with nonsensical item response data, the KR-20 estimate of reliability increases as a function of both true score variance and error variance.  This \em feels \em very wrong.  Increasing true score variance can be done by applying an instrument to a population it may not have been originally designed for.  Increasing error variance can be done by adding more items or manipulating the quality of items.  The challenge with feeling good about the CTT model is that KR-20 is both heavily valued and easily manipulated.  The worst part is that some of the behaviors that would increase a KR-20 value could have negative impacts on the validity of the instrument.

\pagebreak
\section{Different Link Functions}
\subsection{The Default}
<<echo=FALSE, results='hide'>>=
sim_data<-function(b, #item difficulties
                   np=1000,
                   link=function(x) exp(x)/(1+exp(x))
                   )
{
    ni<-length(b)
    th<-rnorm(np)
    th.mat<-matrix(th,np,ni,byrow=FALSE) 
    a<-1
    b.mat<-matrix(b,np,ni,byrow=TRUE) #these are the item difficulties
    ##now the probability of a correct response is:
    pr<-link(a*th.mat+b.mat) ##look very carefully here at how b.mat is being included. is this what you expect? if not, take a look at ?mirt (specifically the form used for the Rash itemtype).
    test<-matrix(runif(ni*np),np,ni)
    resp<-ifelse(pr>test,1,0)
    colnames(resp)<-paste("i",1:ncol(resp))
    resp
}




##first, the default
b<-rnorm(50)
resp<-sim_data(b=b) 
library(mirt)
m<-mirt(resp,1,itemtype="Rasch")
get_coef<-function(mod) {
    coef(mod)->co
    co[-length(co)]->co
    do.call("rbind",co)
}
plot(b,get_coef(m)[,2]); abline(0,1)


@

The default estimates item difficulties (or item easiness, specifically, because mirt) that are in line with the actual (specified) item difficulties.

\subsection{The Normal}

<<echo=FALSE, results='hide'>>=
##now, the normal
b<-rnorm(50)
resp<-sim_data(b=b,link=pnorm) 
m<-mirt(resp,1,itemtype="Rasch")
plot(b,get_coef(m)[,2]); abline(0,1) 

@

Using a normal link function, mirt predicts easy items are easier than they actually are and hard items are harder than they actually are.  The farther an item is from zero, the larger the gap between estimated diffuculty and specified difficulty.

\subsection{Heavy Tails}

<<echo=FALSE, results='hide'>>=
##something with heavy tails
b<-rnorm(50)
resp<-sim_data(b=b,link=function(x) pt(x,df=50)) 
m<-mirt(resp,1,itemtype="Rasch")
plot(b,get_coef(m)[,2]); abline(0,1) 

@

Using a link function with heavy tails, mirt does the same as above.

\subsection{Skewed}

<<echo=FALSE, results='hide'>>=

##skew
library(sn)
x<-seq(-3,3,length.out=100)
plot(x,dsn(x,alpha=3),type="l") #note the skew
plot(x,psn(x,alpha=3),type="l") #corresponding icc
b<-rnorm(50)
resp<-sim_data(b=b,link=function(x) dsn(x,alpha=3)) 
m<-mirt(resp,1,itemtype="Rasch")
plot(b,get_coef(m)[,2]); abline(0,1) #hm, what is going on here? can we make sense of why parameter estimation is quite bad when b is large

@

When using a skewed distribution as the link function, every item is estimated as being harder than it actually is, but where this model really tanks is for extremely easy items.  It finds those to be significantly harder than they actually are and the difference between actual and estimated difficulty for easy items diverges wildly.  The skew on the link function is set up in a way that there is essentially no impact of increased $\theta$ on the respondent's probability of getting the item correct until $\theta$ gets to a value of $-1$.  Remember that the input to the model is of the form $\theta_i + b_j$.  If trait values less than $-1$ don't provide any increase in probability of scoring correctly on an item, for items with easiness above $1$, the model will view those as significantly harder than they actually are.  If you look at the $b_estimated$ vs. $b$ curve, you can see that at $b =1$, the estimated easiness starts to take a turn away from actual easiness.

\end{document}